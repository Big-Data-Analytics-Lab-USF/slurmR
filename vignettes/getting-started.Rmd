---
title: "Getting Started with sluRm"
author: "George G. Vega Yon"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Getting Started with sluRm}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Introduction

Nowadays, high-performance-computing (HPC) clusters are commonly available tools for either in or out of cloud settings. [Slurm Work Manager](https://slurm.schedmd.com/) (formerly *Simple Linux Utility for Resource Manager*) is a program written in C that is used to efficiently manage resources in HPC clusters. The sluRm R package provides tools for using R in HPC settings that work with Slurm. It provides wrappers and auxiliary functions that allow the user to seamlessly integrate their analysis pipeline with HPC, putting emphasis on providing the user with a family of functions similar to those that the parallel R package provides.

First, some important definitions

1.  **Node** A single computer in the HPC.
2.  **Partition** A group of nodes in HPC (see `--partition`).
3.  **Account** Accounts associated with partitions. Accounts can have privileges to use a partition or set of nodes. (See `--account`)
4.  **Job** A job submited to slurm either via `sbatch`, `salloc`, or `srun`.
5.  **Task** A step within a job. A particular job can have multiple tasks.
6.  **CPU** (see `--cpus-per-task`, `--`)

For more information, see https://slurm.schedmd.com/cpu_management.html


# Implementation of the `sluRrm` package

Just like `rslurm`, `sluRm` has two levels of job distribution: first, Slurm Jobs (via
the `Slurm_lapply` and `Slurm_Map` functions), and second, within each job via `parallel::mclapply`
and `parallel::mcMap` (task forking).

In general, the function `Slurm_*` is implemented as follows:

1.  List whatever R packages are loaded, including the path to the R package.

2.  List all the objects passed via elipsis (`...`), and, together with `X` and
    `FUN` or `f`, save them at `[job_path]/[job_name]/` as `[object-name].rds`.

3.  Write out the corresponding R script and Slurm bash file, and save them as
    `[job_path]/[job_name]/00-rscript.r`, and `[job_path]/[job_name]/01-bash.sh`
    respectively.

4.  If `submit = TRUE` (the default), the job will be submitted to the queue,
    which implies that `Slurm_lapply` will call `sbatch()`. Then return.
    
5.  Once `sbatch()` is called, a Job Array will be submitted in which each R
    job will lunch up to `mc.cores` forked processes (2nd layer of parallelization)
    
    
Users can collect their results using the function `Slurm_collect`.
  

# Examples

## Simulating Pi

We would like to implement a simulation algorithm to be run in a cluster. In this
case, we have the very simple function we would like to parallelize:

```r
simpi <- function(n) {
  points <- matrix(runif(n*2), ncol=2)
  mean(rowSums(points^2) <= 1)*4
}
```

This simple function generates an estimate of Pi. This approximation is based on
the following observation

$$
\mbox{Area} = \pi\times r^2 \implies \frac{Area}{r^2} = \pi
$$

Since we know what $r$ is, we just need to get an estimate of the Area to obtain
an approximation of $\pi$. A rather simple way of doing so is with Monte Carlo
simulations, in particular, sampling points in a unit square. The proportion
of points that fall within the unit circle, i.e. the proportion of points which's
distance to the origin is smaller than the radious of the circle, has an expected
value equal to the area of its circunscribed circle (for more details, check out
the wikipedia article about this topic [here](https://en.wikipedia.org/wiki/Approximations_of_%CF%80#Summing_a_circle's_area)).


Using `parallel::mclapply`, we could just type

```r
set.seed(12)
ans <- parallel::mclapply(rep(1e6, 100), simpi)
mean(unlist(ans))
```

Which estiates pi using a single node(computer). However, we can simply exploit parallelisation using `Slurm_lapply`, by writing

```{r slurm-opts, warning=FALSE}
library(sluRm)

# Setting required parameters
opts_sluRm$set_tmp_path(tempdir())
opts_sluRm$set_job_name("test1")

# Optional parameters are set via set_opts
opts_sluRm$set_opts(partition="conti", account="lc_dvc")

# We can look at the setup
opts_sluRm
```


```r
job <- Slurm_lapply(rep(1e6, 100), simpi, njobs=10, mc.cores=10, wait = TRUE)
ans <- Slurm_collect(job)
mean(unlist(ans))
```



```bash
#!/bin/bash
#SBATCH
Rscript --vanilla myjob.R
```

```bash
$ sbatch run_jobs.sh
```

```r
library(sluRm)
library(parallel) # Example package you would like to load

dat <- sample.int(1000, 10, TRUE)
job <- Slurm_lapply(dat, mean, wait = TRUE)

ans <- Slurm_collect(job)
ans
```
# Common options to pass via `sbatch_opt`

Options to `sbatch` can be passed via `sbatch_opt` as a list. For example, the
following

```r
list(
  `job-name`    = "my-fancy-slurm-job",
  `mem-per-cpu` = "4G",
  time          = "12:00:00",
  ntasks        = 10
)
```

Advises Slurm to allocate 4G of memory per task, set a maximum time limit of 12
hours, and specify that you will be running 10 tasks per job.

A comprehensive list of options can be found [here](https://slurm.schedmd.com/sbatch.html).

# Appendix

Extract from the FAQs at the Slurm website:

> **30. Slurm documentation refers to CPUs, cores and threads. What exactly is considered a CPU?**
If your nodes are configured with hyperthreading, then a CPU is equivalent to a hyperthread. Otherwise a CPU is equivalent to a core. You can determine if your nodes have more than one thread per core using the command "scontrol show node" and looking at the values of "ThreadsPerCore".
>
> *Note that even on systems with hyperthreading enabled, the resources will generally be allocated to jobs at the level of a core* (see NOTE below). Two different jobs will not share a core except through the use of a partition OverSubscribe configuration parameter. For example, a job requesting resources for three tasks on a node with ThreadsPerCore=2 will be allocated two full cores. Note that Slurm commands contain a multitude of options to control resource allocation with respect to base boards, sockets, cores and threads. --- [FAQ #30](https://slurm.schedmd.com/faq.html#cpu_count)

