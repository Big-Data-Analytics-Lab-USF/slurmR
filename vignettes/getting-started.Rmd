---
title: "Getting Started with slurmR"
author: "George G. Vega Yon"
date: "June 26, 2019 (last update Oct 31, 2019)"
output:
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 1
vignette: >
  %\VignetteIndexEntry{Getting Started with slurmR}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Introduction

Nowadays, high-performance-computing (HPC) clusters are commonly available tools
for either in or out of cloud settings.
[Slurm Work Manager](https://slurm.schedmd.com/) (formerly
*Simple Linux Utility for Resource Manager*) is a program written in C that is
used to efficiently manage resources in HPC clusters. The slurmR R package
provides tools for using R in HPC settings that work with Slurm. It provides
wrappers and auxiliary functions that allow the user to seamlessly integrate
their analysis pipeline with HPC, putting emphasis on providing the user with a
family of functions similar to those that the parallel R package provides.

First, some important discussion points within the context of Slurm+R that users
in general will find useful. Most of the points have to do with options
available for Slurm, and in particular, with the `sbatch` command with is used
to submit batch jobs to Slurm. Users who have used Slurm in the past may wish
to skip this and continue reading the following section.

*   **Node** A single computer in the HPC: A lot of times jobs will be submitted
    to a single node. The simplest way of using R+Slurm is submitting a single
    job and requesting multiple CPUs to use, for example, `parallel::parLapply`
    or `parallel::mclapply`. Usually users do not need to request an specific
    number of nodes to be used as Slurm will allocate the resources as needed.
    
    A common mistake of R users is to specify the number of nodes and expect
    that their script will be parallelized. This won't happen unless the user
    explicitly writes a parallel computing script.
    
    The relevant flag for `sbatch` is `--nodes`.
    
*   **Partition** A group of nodes in HPC. Generally large nodes may have
    multiple partitions, meaning that nodes may be grouped in various
    ways. For example, nodes belonging to a single group of users may
    be in a single partition, nodes dedicated to work with large data
    may be in another partition. Usually, partitions are associated with
    account privileges, so users may need to specify which account are
    they using when telling Slurm what partition they plan to use.
    
    The relevant flag for `sbatch` is `--partition`.

*   **Account** Accounts may be associated with partitions. Accounts can have
    privileges to use a partition or set of nodes. Often, users need to 
    specify the account when submitting jobs to a particular partition.
    
    The relevant flag for `sbatch` is `--account`.

*   **Task** A step within a job. A particular job can have multiple tasks.
    tasks may span multiple nodes, so if the user wants to submit a multicore
    job, this option may not be the right one.

    The relevant flag for `sbatch` is `--ntasks`

*   **CPU** generally this refers to core or thread (which may be different
    in systems supporting multithreaded cores). Users may want to specify
    how many CPUs they want to use for a task. And this is the relevant
    option when using things like OpenMP or functions that allow creating
    cluster objects in R (e.g. `makePSOCKcluster`, `makeForkCluster`).
    
    The relevant option in `sbatch` is `--cpus-per-task`. More information
    regarding CPUs in Slurm can be found
    [here](https://slurm.schedmd.com/cpu_management.html).
    
*   **Job Array** Slurm supports job arrays. A job array is in simple terms
    a job that is repeated multiple times by Slurm, this is, replicates a
    single job as requested per the user. In the case of R, when using 
    this option, a single R script is spanned in multiple jobs, so the
    user can take advantage of this and parallelize jobs accross multiple
    nodes. Besides from the fact that jobs within a Job Array may be spanned
    accross multiple nodes, each job in that array has a unique ID that
    is available to the user via environment variables, in particular
    `SLURM_ARRAY_TASK_ID`.
    
    Within R, and hence the Rscript submitted to Slurm, users can access
    this environment variable with `Sys.getenv("SLURM_ARRAY_TASK_ID")`.
    Some of the functionalities of `slurmR` rely on Job Arrays.
    
    More information on Job Arrays can be found
    [here](https://slurm.schedmd.com/job_array.html).
    The relevant option for this in `sbatch` is `--array`.
    
More information about Slurm can be found their official website
[here](https://slurm.schedmd.com/). A tutorial about how to use Slurm with R
can be found [here](https://uscbiostats.github.io/slurmr-workshop).

# Submitting jobs via sbatch

In general, users will submit jobs to Slurm using the `sbatch` command line
function. The `sbatch` function's main argument is the name (path) to a bash
script that holds the instructions (and sometimes options) associated to
the program. Here is an example of an bash file to be submitted to Slurm

```bash
#!/bin/bash
#SBATCH --time=01:00:00
#SBATCH --job-name="A long job"
#SBATCH --mem=5GB
#SBATCH --output=long-job.out
cd /path/where/to/start/the/job

# This may vary per HPC system. At USC's hpc system
# we use: source /usr/usc/R/default/setup.sh
module load R

Rscript --vanilla long-job-rscript.R
```

This example bash file, wich we name "long-job-rscript.slurm", has the following
components:

*  `#!/bin/bash` The interpreter directive that is common to bash scripts.
   For more on this see [this thread on StackExchange](https://askubuntu.com/questions/141928/what-is-the-difference-between-bin-sh-and-bin-bash).

*  The `#SBATCH` lines specify options for scheduling the job. In order, these
   options are: Set a maximum time of 1 hour, name the job `A long job`, allocate
   5GB of memory to the job, write all the output (including `Rscript`'s) to
   `long-job.out`.

*  The `cd` line changes the directory to some other place where the Rscript
   needs to be executed.
   
*  The `module` line loads R. There are various ways to do this, but it is
   a common requirement for the user to specify that it will be using R.
   
*  Finally, `Rscript` executes the R script named `long-job-rscript.R`.

This batch script can be submitted to Slurm using the `sbatch` command line tool:

```bash
$ sbatch long-job-rscript.slurm
```

This is what happens under-the-hood in `slurmR` overall. The next section will
discuss the ways in which `slurmR` enables out-of-the-box parallel computing
using Slurm.

# Implementation of the `slurmR` package

## *apply family

Just like `rslurm`, `slurmR` has two levels of job distribution: first, Slurm Jobs (via
the `Slurm_lapply` and `Slurm_Map` functions), and second, within each job via `parallel::mclapply`
and `parallel::mcMap` (task forking).

In general, the function `Slurm_*` is implemented as follows:

1.  List whatever R packages are loaded, including the path to the R package.

2.  List all the objects passed via ellipsis (`...`), and, together with `X` and
    `FUN` or `f`, save them at `[job_path]/[job_name]/` as `[object-name].rds`.

3.  Write out the corresponding R script and Slurm bash file, and save them as
    `[job_path]/[job_name]/00-rscript.r`, and `[job_path]/[job_name]/01-bash.sh`
    respectively.

4.  If `submit = TRUE` (the default), the job will be submitted to the queue,
    which implies that `Slurm_lapply` will call `sbatch()`. Then return.
    
5.  Once `sbatch()` is called, a Job Array will be submitted in which each R
    job will lunch up to `mc.cores` forked processes (2nd layer of palatalization)
    
Users can collect their results using the function `Slurm_collect`. Furthermore
The advantage of using this approach instead of using the `makeSlurmCluster`
wrapper (which is discussed later) is that job-arrays can be re-submitted and,
furthermore, the user can chose to not wait for the job to wait, and thus, quit
the R session after submitting jobs to the queue.

## Integration with the parallel package

Another important component of `slurmR` is `makeSlurmCluster` function. This
allow users creating multi-node PSOCKCluster class objects. The implementation
of this function, wrapper of `parallel::makePSOCKcluster`, is very simple:

1. It submits a job to Slurm requesting the desired number of tasks. Each task
will then return information regarding the node at which it is operating.

2. Once Slurm allocates the resources, the master R session (from which the
job was submitted) will read in the node names returned by each task.

3. With the full list of nodenames in usage, `makeSlurmCluster` will pass the
list of names to `parallel::makePSOCKcluster`, which ultimately creates the
`cluster` class object.

After creating the cluster object, the workflow is exactly the same as with
the `parallel` package. Here is an example from the `makeSlurmCluster`
manual

```r
# Creating a cluster with 100 workers/offpring/child R sessions
cl <- makeSlurmCluster(100)

# Computing the mean of a 100 random uniforms within each worker
# for this we can use any of the function available in the parallel package.
ans <- parSapply(1:200, function(x) mean(runif(100)))

# We simply call stopCluster as we would do with any other cluster
# object
stopCluster(ans)
```

# Example simulating Pi

We would like to implement a simulation algorithm to be run in a cluster. In this
case, we have the very simple function we would like to parallelize:

```r
simpi <- function(n) {
  points <- matrix(runif(n*2), ncol=2)
  mean(rowSums(points^2) <= 1)*4
}
```

This simple function generates an estimate of Pi. This approximation is based on
the following observation

$$
\mbox{Area} = \pi\times r^2 \implies \frac{Area}{r^2} = \pi
$$

Since we know what $r$ is, we just need to get an estimate of the Area to obtain
an approximation of $\pi$. A rather simple way of doing so is with Monte Carlo
simulations, in particular, sampling points in a unit square. The proportion
of points that fall within the unit circle, i.e. the proportion of points whose
distance to the origin is smaller than the radius of the circle, has an expected
value equal to the area of its circumscribed circle (for more details, check out
the Wikipedia article about this topic [here](https://en.wikipedia.org/wiki/Approximations_of_%CF%80#Summing_a_circle's_area)).


Using `parallel::mclapply`, we could just type

```r
set.seed(12)
ans <- parallel::mclapply(rep(1e6, 100), simpi)
mean(unlist(ans))
```

Which estimates pi using a single node(computer). However, we can simply exploit parallelization using `Slurm_lapply`, by writing

```{r slurm-opts, warning=FALSE}
library(slurmR)

# Setting required parameters
opts_slurmR$set_tmp_path(tempdir())
opts_slurmR$set_job_name("test1")

# Optional parameters are set via set_opts
opts_slurmR$set_opts(partition="conti", account="lc_dvc")

# We can look at the setup
opts_slurmR
```


```r
job <- Slurm_lapply(rep(1e6, 100), simpi, njobs=10, mc.cores=10, plan = "wait")
ans <- Slurm_collect(job)
mean(unlist(ans))
```

# Common options to pass via `sbatch_opt`

Options to `sbatch` can be passed via `sbatch_opt` as a list. For example, the
following

```r
list(
  `job-name`      = "my-fancy-slurm-job",
  `mem-per-cpu`   = "4G",
  time            = "12:00:00",
  `cpus-per-task` = 10
)
```

Advises Slurm to allocate 4G of memory per task, set a maximum time limit of 12
hours, and specify that you will be using 10 CPUs.

A comprehensive list of options can be found
[here](https://slurm.schedmd.com/sbatch.html).

# Appendix

Extract from the FAQs at the Slurm website:

> **30. Slurm documentation refers to CPUs, cores and threads. What exactly is considered a CPU?**
If your nodes are configured with hyperthreading, then a CPU is equivalent to a hyperthread. Otherwise a CPU is equivalent to a core. You can determine if your nodes have more than one thread per core using the command "scontrol show node" and looking at the values of "ThreadsPerCore".
>
> *Note that even on systems with hyperthreading enabled, the resources will generally be allocated to jobs at the level of a core* (see NOTE below). Two different jobs will not share a core except through the use of a partition OverSubscribe configuration parameter. For example, a job requesting resources for three tasks on a node with ThreadsPerCore=2 will be allocated two full cores. Note that Slurm commands contain a multitude of options to control resource allocation with respect to base boards, sockets, cores and threads. --- [FAQ #30](https://slurm.schedmd.com/faq.html#cpu_count)

